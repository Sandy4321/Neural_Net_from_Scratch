{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a d-layer Neural Network from Scratch to Classify MNIST\n",
    "### by Eduardo Coronado (12/15/19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will construct a $d$-dimensional neural network with the following architecture 784 − $H$ −\n",
    "$H$ - $\\ldots$ − 1, where $H$ is the number of hidden nodes (neurons) per layer $d$ is the number of layers. We will use this to classify the number from the MNIST data set and show we can achive $>97\\%$ accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "### Load and Extract\n",
    "\n",
    "First, we load the pre-processed MNIST data found in this repo `mnist_all.mat` using the `sio` package. Where the pre-processing involved randomly splitting the original dataset into train and test sets and turning each image $\\mathbf{x}_i$ into a $1x 784$ vector (i.e. $f : \\mathbb{R}^{28x28} \\rightarrow \\mathbb{R}^{784}$).  \n",
    "\n",
    "We then extract the train and test data per digit and store them into numpy arrays. The features `X_train` and `X_test` were normalized such that each value is between 0 and 1. Since MNIST images are black and white we can achive this by dividing $1\\;/ \\;255$, given $255$ is the max value in this type of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "mnist = sio.loadmat(\"./mnist_all.mat\")\n",
    "\n",
    "# Extract pixel data/observations\n",
    "X_test = [val for key, val in mnist.items() if \"test\" in key]\n",
    "X_train = [val for key, val in mnist.items() if \"train\" in key]\n",
    "\n",
    "# Normalize (1/255) given MNIST data is B&W images\n",
    "X_test = np.vstack(X_test) / 255.\n",
    "X_train = np.vstack(X_train)/ 255.\n",
    "X_train, X_test = X_train.T, X_test.T\n",
    "\n",
    "# Extract observation train and test labels observations\n",
    "y_test = [np.full((1,val.shape[0]),key.replace(\"test\",\"\"), dtype=int ) \n",
    "          for key,val in mnist.items() if \"test\" in key]\n",
    "y_train = [np.full((1,val.shape[0]),key.replace(\"train\",\"\"), dtype=int ) \n",
    "          for key,val in mnist.items() if \"train\" in key]\n",
    "\n",
    "y_test = np.hstack(y_test)\n",
    "y_train = np.hstack(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode labels\n",
    "Since the MNIST data set contains 10 digits (i.e. 10 different labels), it is beneficial to one-hot encode the labels such that we obtain a $10xn$ and $10xm$ arrays for $n$ train and $m$ test labels. Here, each column represents the label for a single image and is sparse. For example, let $\\mathbf{x}_j = \\{x_{1j}, x_{2j}, \\ldots, x_{nj}\\}$ be the one-hot encoded label for observation $j$ and $x_{\\cdot j} \\in \\{0,1\\}$ be the entries of this column. So, if this observation's was labeled as digit 0 then $x_{1j} = 1$ and $x_{i,j} = 0  \\quad \\text{for }\\; i= 2,\\ldots,10$\n",
    "\n",
    "Thus  $\\mathbf{x}_j = \\{1, 0, \\ldots, 0\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode train and test labels 10 x n_samples with 0,1 entries\n",
    "y_test_new = np.eye(10)[y_test.astype('int32')]\n",
    "y_test_new = y_test_new.T.reshape(10, y_test.shape[1])\n",
    "\n",
    "y_train_new = np.eye(10)[y_train.astype('int32')]\n",
    "y_train_new = y_train_new.T.reshape(10, y_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Neural Network\n",
    "Before we dive into any further coding, we will go over some basics.\n",
    "\n",
    "First, let's define the networks high-level architecture such as the layer weights $w_j$, layer outputs $h_j$,  activations functions $\\sigma$, and more.\n",
    "\n",
    "We can represent the neural network as the following function \n",
    "\n",
    "$$f(x \\mid \\theta) : \\mathbb{R}^{784} \\rightarrow \\{0,1\\}$$, \n",
    "\n",
    "where the parameters $\\theta$ of a network with $d$ layers is defined by\n",
    "\n",
    "$$\\theta = \\{ \\mathbf{W}_1 \\in \\mathbb{R}^{784 x H}, \\mathbf{W}_j \\in \\mathbb{R}^{H x H}, \\mathbf{w}_d \\in \\mathbb{R}^H, \\mathbf{b}_k \\in \\mathbb{R}\\}$$\n",
    "\n",
    "where $k = 1, \\ldots, d$ and $ 1< j <d$.\n",
    "\n",
    "More explicitly, $f(x)$ is defined by the following\n",
    "\\begin{align*}\n",
    "f(x) &= \\sigma_m \\left(\\mathbf{w}_d^T h_{d-1} + \\mathbf{b}_d \\right)\\\\[1ex]\n",
    "h_{d-1} &= \\sigma \\left(\\mathbf{W}_{d-1}^T h_{d-2} + \\mathbf{b}_{d-1} \\right)\\\\[1ex]\n",
    "&\\vdots\\\\[1ex]\n",
    "h_2 &= \\sigma \\left(\\mathbf{W}_2^T h_1 + \\mathbf{b}_2 \\right)\\\\[1ex]\n",
    "h_1 &= \\sigma \\left(\\mathbf{W}_1^T \\mathbf{x} + \\mathbf{b}_1\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{x}$ is the input data, $b_i$ are the bias per layer, \n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma(\\mathbf{z}) &= \\frac{1}{1 + e^{-z}} \\quad \\text{(is a } \\mathbf{sigmoid \\; activation} \\text{ function)}\\\\[1ex] \\sigma_m(\\mathbf{z})_i &= \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\quad \\text{(is a } \\mathbf{softmax \\; activation} \\text{ function to classify MNIST 10 digits (i.e. $K = 10$, 0 through 9 digits))}\n",
    "\\end{align*}\n",
    "\n",
    "Similarly, we will use a **cross-entropy** loss function \n",
    "\n",
    "$$\\mathcal{L}(\\theta) = - \\sum_{i=1}^N (y_i log f(\\mathbf{x}_i) + (1 - y_i)log(1 - f(\\mathbf{x}_i))$$\n",
    "\n",
    "Having define the above we can now define some helper functions for the loss and activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some helper functions\n",
    "\n",
    "# Sigmoid activation\n",
    "def sigmoid(z):\n",
    "    out = 1. / (1. + np.exp(-z))\n",
    "    return out\n",
    "\n",
    "# Softmax output activation for final layer(I decided to use this given build a multi-class NN)\n",
    "def softmax(z):\n",
    "    out_soft = np.exp(z)/ np.sum(np.exp(z), axis = 0)\n",
    "    return out_soft\n",
    "\n",
    "# Loss function: cross entropy\n",
    "def cross_entropy(y_true, y_hat):\n",
    "    loss_sum = np.sum(np.multiply(np.log(y_hat),y_true) ) + np.sum( np.multiply(np.log(1-y_hat),(1-y_true)))\n",
    "    loss = - (1./y_true.shape[1]) * loss_sum\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main steps to train a Neural Network\n",
    "\n",
    "Training a Neural Network involves three main steps,\n",
    "\n",
    "1) **Initiate Parameters**: Randomly initiate weights $\\mathbf{W}$ and biases $\\mathbf{b}$\n",
    "\n",
    "2) **Feedforward**: Compute the errors based on the predicted label assigments for each observation \n",
    "\n",
    "3) **Backpropagation**: Compute weight gradients to adjust weights based on error and loss function\n",
    "\n",
    "**Repeat steps 2-3 above iteratively to generate a learning process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Parameter Initialization\n",
    "\n",
    "The first step before training a neural network is to initialize the weight and bias parameters for the number of layers and neurons/layer in the network. In this example, I've built a function that will initialize a neural net with $d$ layers (user defined) each with a default 64 neurons/layer and will output a dictionary `param` \n",
    "that will contain the weight $\\mathbf{W}$ and bias $\\mathbf{b}$ parameter values.\n",
    "\n",
    "A common way to do so is randomly initializing $\\mathbf{W} \\sim N(0, \\sigma^2)$ and $\\mathbf{b} = \\mathbf{0}$. However, it is important to choose a value of $\\sigma^2$ such that the weights are similar in distribution for all layers and **avoid problems with vanishing or exploding gradients** later on. In other words, we'd like to achieve\n",
    "\n",
    "$$Var(\\mathbf{z}_l) = Var(\\mathbf{z}_{l-1})$$\n",
    "\n",
    "where $\\mathbf{z}_l = \\mathbf{W}_l^T \\mathbf{h}_{l-1} + \\mathbf{b}_l$.\n",
    "\n",
    "After some simplification from first principles and taking into account that at each layer the weights have a symmetric, i.i.d. normal distribution we obtain the following optimal value for variance $\\sigma^2$\n",
    "\n",
    "$$\\sigma^2_l = \\frac{2}{n_l}$$\n",
    "\n",
    "\n",
    "Therefore, during initialization we should make sure that the variance for each weight is $\\frac{2}{n_l}$ which can be obtained by multiplying the $\\mathbf{W}_l \\sim N(0,1)$ random variable by $\\sqrt{\\frac{2}{n_l}}$ to obtain the desired variance. This is based on the quadratic properties of the variance of an i.i.d. random variable - $Var(aX) = a^2Var(X)$.\n",
    "<br><br>\n",
    "\n",
    "In this example, the $n_1 = 784$ and each layer after $n_2 = \\ldots = n_d = 64$ since we are using vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize layer parameters\n",
    "# Choose n_h = number of hidden nodes, \n",
    "# d = number of layers, \n",
    "# n_x = number of inputs per observation (i.e. 784 for MNIST)\n",
    "def initialize_params(layers, n_h = 64, n_x = 784):\n",
    "    np.random.seed(10)\n",
    "    param = {}\n",
    "    \n",
    "    for i in range(1, layers + 1):\n",
    "        w_str, b_str = \"w\" + str(i), \"b\" + str(i)\n",
    "        \n",
    "        # First layer\n",
    "        if i == 1:\n",
    "            # Use weight normalization after random sample from std normal based on Var derivation in part b\n",
    "            param[w_str] = np.random.randn(n_h, n_x) * np.sqrt((2./ n_x)) \n",
    "            \n",
    "            param[b_str] = np.zeros((n_h, 1)) * np.sqrt(2. / n_x)\n",
    "            \n",
    "        # Last layer\n",
    "        elif i == d:\n",
    "            param[w_str] = np.random.randn(10, n_h) * np.sqrt((2./ n_h))\n",
    "            param[b_str] = np.zeros((10, 1)) * np.sqrt(2. / n_x)\n",
    "        \n",
    "        # Any intermediate layer\n",
    "        else:\n",
    "            param[w_str] = np.random.randn(n_h, n_h) * np.sqrt((2./ n_h))\n",
    "            param[b_str] = np.zeros((n_h, 1)) * np.sqrt(2. / n_h)\n",
    "            \n",
    "    return param\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Feedforward\n",
    "\n",
    "Next, lets define the feedforward step based on the layout above. Here we compute the following to obtain the value of $f(\\mathbf{x})$ for a single input $\\mathbf{x}$.\n",
    "\n",
    "\\begin{align*}\n",
    "h_1 &= \\sigma \\left(\\mathbf{W}_1^T \\mathbf{x} + \\mathbf{b}_1\\right)\\\\[1ex]\n",
    "h_2 &= \\sigma \\left(\\mathbf{W}_2^T h_1 + \\mathbf{b}_2 \\right)\\\\[1ex]\n",
    "&\\vdots\\\\[1ex]\n",
    "h_{d-1} &= \\sigma \\left(\\mathbf{W}_{d-1}^T h_{d-2} + \\mathbf{b}_{d-1} \\right)\\\\[1ex]\n",
    "f(x) &= \\sigma_m \\left(\\mathbf{w}_d^T h_{d-1} + \\mathbf{b}_d \\right)\\\\[1ex]\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma$ is the sigmoid function and $\\sigma_m$ is the softmax function.\n",
    "\n",
    "In this example, we store this values in `dictionaries` to make the values easy to access. Let $\\mathbf{z}$ be the inputs into the activation function $\\sigma(\\mathbf{z})$ coming from the previous layer, and $h$ be the output after the activation function in the current layer (i.e. $\\mathbf{h} = \\sigma(\\mathbf{z})$).  \n",
    "\n",
    "Here we define `fwd_dic` which the dictionary that stores all the layer inputs $\\mathbf{z}_j$ and outputs $\\mathbf{h}_j$, and leverage the `param` dictionary we used during initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward function\n",
    "# Inputs: initialized param dictionary, X matrix (784 x n), and d layers\n",
    "# Output: dictionary with forward computations for z_j and h_j activations\n",
    "def forward(param, X, d):\n",
    "    fwd_dic = {} # Initialize dictionary\n",
    "    \n",
    "    for i in range(1, d+1):\n",
    "        z_str = \"z\" + str(i) # create z_j for activation functions h_j\n",
    "        h_str = \"h\" + str(i) # activation z_j\n",
    "        w_str, b_str = \"w\" + str(i), \"b\" + str(i) \n",
    "        \n",
    "        # First layer computations z_1 and h_1\n",
    "        if i == 1:\n",
    "            fwd_dic[z_str] = np.matmul(param[w_str], X) + param[b_str]\n",
    "            fwd_dic[h_str] = sigmoid(fwd_dic[z_str])\n",
    "            \n",
    "        # Final layer computations z_d and h_d\n",
    "        elif i == d:\n",
    "            # Previous layer's activation\n",
    "            h_str_prev = \"h\" + str(i -1)\n",
    "            prev_h = fwd_dic[h_str_prev]\n",
    "            \n",
    "            # Current layer activation\n",
    "            fwd_dic[z_str] = np.matmul(param[w_str], prev_h) + param[b_str]\n",
    "            fwd_dic[h_str] = softmax(fwd_dic[z_str]) # Softmax activation for multi-class\n",
    "        \n",
    "        # Intermediate layer computations\n",
    "        else:\n",
    "            # Previous layer's activation\n",
    "            h_str_prev = \"h\" + str(i -1)\n",
    "            prev_h = fwd_dic[h_str_prev]\n",
    "            \n",
    "            # Current layer activation\n",
    "            fwd_dic[z_str] = np.matmul(param[w_str], prev_h) + param[b_str]\n",
    "            fwd_dic[h_str] = sigmoid(fwd_dic[z_str])\n",
    "      \n",
    "    \n",
    "    return fwd_dic\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Backpropagation\n",
    "\n",
    "After we compute the backpropagation step. Here we want to learn know how $\\mathcal{L}$ changes with respect to each weight $\\mathbf{W}_j$ and bias $\\mathbf{b}_k$ for $j,k = 1, \\ldots, d$. \n",
    "\n",
    "The learning process is normally done via gradient descent, however since the MNIST data set is large we can approximate the true gradient using **Stochastic Gradient Descent** which can be computed \n",
    "\n",
    "$$\\nabla \\mathcal{L} = \\frac{1}{B} \\sum_{b =1}^B \\nabla \\mathcal{L}_b$$\n",
    "\n",
    "where at each step we randomly pick a mini-batch of $B$ samples and calculate the gradient based on this mini-batch.\n",
    "\n",
    "#### Derive Gradients  and Stochastic Gradient Descent\n",
    "\n",
    "Since we're interested in how the loss changes with respect to the weights and biases we can leverage the chain rule\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{W}_j} \\mathcal{L}(\\theta) &= \\frac{d \\mathcal{L}(\\theta)}{d \\mathbf{h}_j} \\frac{d \\mathbf{h}_j}{d \\mathbf{z}_j}\\frac{d \\mathbf{z}_j}{d \\mathbf{W}_j}\\\\[1ex]\n",
    "\\nabla_{\\mathbf{b}_j} \\mathcal{L}(\\theta) &= \\frac{d \\mathcal{L}(\\theta)}{d \\mathbf{h}_j} \\frac{d \\mathbf{h}_j}{d \\mathbf{z}_j}\\frac{d \\mathbf{z}_j}{d \\mathbf{b}_j}\n",
    "\\end{align*}\n",
    "\n",
    "As an example, let's focus on the gradients of the $d^{th}$ layer. Let $\\mathbf{z}_d = \\mathbf{W}_d^T \\mathbf{h}_{d-1} + \\mathbf{b}_d$, $f(\\mathbf{x}) = \\sigma(\\mathbf{z}_d)$, and $\\sigma^\\prime(\\mathbf{z}) = \\sigma(\\mathbf{z})(1 - \\sigma(\\mathbf{z}))$. Then based on the above configuration,\n",
    "\n",
    "\\begin{align*}\n",
    " \\frac{d \\mathcal{L}(\\theta)}{d \\sigma(\\mathbf{z}_d)} \\frac{d \\sigma(\\mathbf{z}_d) }{d\\mathbf{z}_d} &= - \\sum_{i=1}^N (y_i (1 - \\sigma(\\mathbf{z}_d)) - (1- y_i) \\sigma(\\mathbf{z}_d)) \\quad \\text{ and }\\\\[1ex]\n",
    "\\frac{d \\mathbf{z}_d}{d \\mathbf{W}_d} &= \\mathbf{h}_{d-1}\\\\[2ex]\n",
    "\\therefore \\quad \\nabla_{\\mathbf{W}_j} (\\theta) &= - \\sum_{i=1}^N (y_i - \\sigma(\\mathbf{z}_d)) \\mathbf{h}_{d-1}\n",
    "\\end{align*}\n",
    "\n",
    "Similarly, we can leverage the first two derivatives above to find  \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{b}_d} \\mathcal{L}(\\theta) &= \\frac{d \\mathcal{L}(\\theta)}{d \\sigma(\\mathbf{z}_d)} \\frac{d \\sigma(\\mathbf{z}_d) }{d\\mathbf{z}_d} \\frac{d \\mathbf{z}_d}{d \\mathbf{b}_d} \\\\[1ex]\n",
    "&= - \\sum_{i=1}^N (y_i - \\sigma(\\mathbf{z}_d)) *1\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "#### Parameter Updates\n",
    "\n",
    "After we've computed the gradients we can then update the weights and biases at iteration $t+1$ via the following,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{W}_j^{t+1} &= \\mathbf{W}_j^{t} - \\alpha \\nabla_{\\mathbf{W}_j} \\mathcal{L}(\\theta)\\\\[1ex]\n",
    "\\mathbf{b}_j^{t+1}&=\\mathbf{b}_j^{t} - \\alpha \\nabla_{\\mathbf{b}_j} \\mathcal{L}(\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "In this example, `mb_sgd` is a dictionary that stores the weigth and bias gradients while the `inter_grads` stores the intermediate chain rule gradients with respect to $d \\mathbf{z}_j$  and $d \\mathbf{h}_j$\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation function\n",
    "# Inputs: fwd_dat dictionary from fwd calcs, B = size of mini-batch, d = layers,\n",
    "# param = initialized params data, X,Y data\n",
    "def backprop(fwd_dat, param,  B, d,  X, Y):\n",
    "    mb_sgd = {} # Final output dictionary w/ dw_j and db_j\n",
    "    inter_grads = {} # Intermediate dict for dz_j and dh_j gradients\n",
    "    \n",
    "    for i in range(d, 0, -1):\n",
    "        dw_str = \"dw\" + str(i) # gradient for w_j\n",
    "        db_str = \"db\" + str(i) # gradient for b_j\n",
    "        dz_str, dh_str = \"dz\" + str(i), \"dh\" + str(i)\n",
    "        z_str, h_str = \"z\" + str(i), \"h\" + str(i)\n",
    "        \n",
    "        \n",
    "        # End layer computation dw and db\n",
    "        if i == d:\n",
    "            # compute dz of final layer\n",
    "            inter_grads[dz_str] = fwd_dat[h_str] - Y\n",
    "            \n",
    "            h_str_prev = \"h\"+ str(i -1)\n",
    "            h_prev = fwd_dat[h_str_prev]\n",
    "            \n",
    "            # compute dw and db from final layer\n",
    "            mb_sgd[dw_str] = (1./B) * np.matmul(inter_grads[dz_str], h_prev.T)\n",
    "            mb_sgd[db_str] = (1./B) * np.sum(inter_grads[dz_str], axis=1, keepdims=True)\n",
    "            \n",
    "            \n",
    "        # First layer dw and db computations\n",
    "        elif i == 1:\n",
    "            \n",
    "            # Compute dh and dz from second layer\n",
    "            w_str_fwd = \"w\" + str(i + 1)\n",
    "            dz_str_fwd = \"dz\" + str(i + 1)\n",
    "            \n",
    "            inter_grads[dh_str] = np.matmul(param[w_str_fwd].T, inter_grads[dz_str_fwd])\n",
    "            inter_grads[dz_str] = inter_grads[dh_str]*sigmoid(fwd_dat[z_str]) * (1 - sigmoid(fwd_dat[z_str]))\n",
    "            \n",
    "            # Compute dw and db for first layer\n",
    "            mb_sgd[dw_str] = (1. / B)* np.matmul(inter_grads[dz_str], X.T)\n",
    "            mb_sgd[db_str] = (1. / B)* np.sum(inter_grads[dz_str], axis = 1, keepdims=True)\n",
    "    \n",
    "        \n",
    "        # Intermediate layer computations dw and db\n",
    "        else:\n",
    "            \n",
    "            # Compute dz and dh from layers in front\n",
    "            w_str_fwd = \"w\" + str(i + 1)\n",
    "            dz_str_fwd = \"dz\" + str(i + 1)\n",
    "            \n",
    "            inter_grads[dh_str] = np.matmul(param[w_str_fwd].T, inter_grads[dz_str_fwd])\n",
    "            inter_grads[dz_str] = inter_grads[dh_str]*sigmoid(fwd_dat[z_str]) * (1 - sigmoid(fwd_dat[z_str]))\n",
    "            \n",
    "            # Backprop towards layers behind to compute dw and db\n",
    "            h_str_prev = \"h\"+ str(i -1)\n",
    "            h_prev = fwd_dat[h_str_prev]\n",
    "            \n",
    "            # Compute dw and db\n",
    "            mb_sgd[dw_str] = (1. / B) * np.matmul(inter_grads[dz_str], h_prev.T)\n",
    "            mb_sgd[db_str] = (1. / B) * np.sum(inter_grads[dz_str], axis=1, keepdims=True)\n",
    "        \n",
    "      \n",
    "    return mb_sgd\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network\n",
    "\n",
    "For this example I built a 3-layered Neural Network (input layer + 1 hidden layer + output layer) where each layer has 64 neurons (these can be changed by modifying `d`, and modifying the `n_h` parameter in the `initialize_params` function).\n",
    "\n",
    "The network was trained over 40 `epochs`, with a mini `batch_size` of 500 for the backprop step, with a `learning_rate` of 1. \n",
    "\n",
    "The train and test loss is printed every 5 epochs (shown below). As we can see, both losses are decreasing as the number of epochs increases which indicates the network is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: test loss = 0.818925043526, train loss = 0.846208574588\n",
      "Epoch 6: test loss = 0.348094883373, train loss = 0.34890110104\n",
      "Epoch 11: test loss = 0.252851299231, train loss = 0.238522267701\n",
      "Epoch 16: test loss = 0.20601395031, train loss = 0.180581588354\n",
      "Epoch 21: test loss = 0.178653547647, train loss = 0.143960062014\n",
      "Epoch 26: test loss = 0.161516956541, train loss = 0.117924419292\n",
      "Epoch 31: test loss = 0.150527946295, train loss = 0.0980818395856\n",
      "Epoch 36: test loss = 0.14358941241, train loss = 0.0824589133178\n"
     ]
    }
   ],
   "source": [
    "# Use helper function to initialize weights and biases into dictionary\n",
    "d = 3 # Includes first layer and final layer (i.e. d = 3 is a 2 hidden layer NN)\n",
    "start_params = initialize_params(layers= d)\n",
    "\n",
    "\n",
    "epochs = 40 # Set number of epochs\n",
    "batch_size = 500 # Set batch-size\n",
    "learning_rate = 1. # set learning rate\n",
    "np.random.seed(10)\n",
    "\n",
    "# Since X_train, y_train are in order, shuffle to avoid any potential bias\n",
    "perm = np.random.permutation(X_train.shape[1])\n",
    "X_train_perm = X_train[:, perm]\n",
    "y_train_perm = y_train_new[:, perm]\n",
    "\n",
    "# For each epoch\n",
    "for i in range(epochs):\n",
    "     \n",
    "    # Loop through all data, split into batch_size defined mini-batches\n",
    "    # to estimate gradients\n",
    "    for j in range(X_train.shape[1] / batch_size):\n",
    "\n",
    "        # Get mini batch\n",
    "        start = j * batch_size # get start idx\n",
    "        end = min(start + batch_size, X_train.shape[1] - 1) # get end of batch\n",
    "        X_mb = X_train_perm[:, start:end]\n",
    "        y_mb = y_train_perm[:, start:end]\n",
    "\n",
    "        # Calculate current fwd data and gradients\n",
    "        fwd_dat = forward(start_params, X_mb, d)\n",
    "        mb_sgd = backprop(fwd_dat, start_params, batch_size, d, X_mb, y_mb )\n",
    "\n",
    "        # Based on gradients from backprop, update weights and biases\n",
    "        for k in range(1, d +1):\n",
    "            w_str, b_str = \"w\" + str(k), \"b\" + str(k)\n",
    "            dw_str, db_str = \"dw\" + str(k), \"db\" + str(k)\n",
    "            \n",
    "            # Update w_j and b_j\n",
    "            start_params[w_str] = start_params[w_str] - learning_rate * mb_sgd[dw_str]\n",
    "            start_params[b_str] = start_params[b_str] - learning_rate * mb_sgd[db_str]\n",
    "\n",
    "\n",
    "    # Calculate training set accuracy\n",
    "    fwd_dat = forward(start_params, X_train, d)\n",
    "    y_pred = fwd_dat[\"h\" + str(d)]\n",
    "    train_loss = cross_entropy(y_train_new, y_pred)\n",
    "\n",
    "    # Calculate training set accuracy\n",
    "    fwd_dat = forward(start_params, X_test, d)\n",
    "    y_pred = fwd_dat[\"h\" + str(d)]\n",
    "    test_loss = cross_entropy(y_test_new, y_pred)\n",
    "    \n",
    "    # Print train and test losses every 5 epochs\n",
    "    if i % 5 == 0:\n",
    "        print(\"Epoch {}: test loss = {}, train loss = {}\".format(i + 1, test_loss, train_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions\n",
    "\n",
    "After the network is trained we compute the forwardfeed step with the most recent weights and biases, and extract the $arg \\; max$ $f(\\mathbf{x})$ of the softmax activation (i.e. the output layer for a single input $\\mathbf{x}$). \n",
    "\n",
    "This will define the classification per each input $\\mathbf{x}$, and thus the classifications for all inputs in $\\mathbf{X}_{train}$ and $\\mathbf{X}_{test}$. `y_pred_train` and `y_pred_test` are $n$ and $m$ length vectors, respectively, of in-sample and out-of-sample predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data predictions\n",
    "fwd_dat_train = forward(start_params, X_train, d)\n",
    "y_pred_train = np.argmax(fwd_dat_train[\"h\" + str(d)], axis=0)\n",
    "\n",
    "# Test data predictions\n",
    "fwd_dat_test = forward(start_params, X_test, d)\n",
    "y_pred_test = np.argmax(fwd_dat_test[\"h\" + str(d)], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Performance\n",
    "\n",
    "We then assess the networks in-sample and out-of-sample performance.\n",
    "\n",
    "### Overall Accuracy\n",
    "\n",
    "First, we compute the overall accuracy and see that in-sample performance is $99\\%$ while the out-of-sample performance is $>97\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990183</td>\n",
       "      <td>0.9746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Training Acc  Test Acc\n",
       "0      0.990183    0.9746"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = accuracy_score(y_train[0], y_pred_train) # Training accuracy\n",
    "test_acc = accuracy_score(y_test[0], y_pred_test) # Test accuracy\n",
    "\n",
    "# Report training/test accuracy\n",
    "pd.DataFrame(np.array([train_acc, test_acc]).reshape(1,2),\n",
    "            columns = [\"Training Acc\", \"Test Acc\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data classification report\n",
    "Looking that the f1-score, we can see that the the network provides good in-sample classifiction across all digits.\n",
    "\n",
    "We look at the f1-scre since given ti provides an overall performance metric for the algorithm that takes into account precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>micro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.993413</td>\n",
       "      <td>0.993100</td>\n",
       "      <td>0.989951</td>\n",
       "      <td>0.988468</td>\n",
       "      <td>0.988464</td>\n",
       "      <td>0.989708</td>\n",
       "      <td>0.993334</td>\n",
       "      <td>0.989641</td>\n",
       "      <td>0.989315</td>\n",
       "      <td>0.986076</td>\n",
       "      <td>0.990147</td>\n",
       "      <td>0.990183</td>\n",
       "      <td>0.990184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.993749</td>\n",
       "      <td>0.993469</td>\n",
       "      <td>0.987801</td>\n",
       "      <td>0.991306</td>\n",
       "      <td>0.986862</td>\n",
       "      <td>0.994966</td>\n",
       "      <td>0.992078</td>\n",
       "      <td>0.988067</td>\n",
       "      <td>0.989569</td>\n",
       "      <td>0.984095</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>0.990183</td>\n",
       "      <td>0.990194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.993078</td>\n",
       "      <td>0.992732</td>\n",
       "      <td>0.992111</td>\n",
       "      <td>0.985647</td>\n",
       "      <td>0.990072</td>\n",
       "      <td>0.984505</td>\n",
       "      <td>0.994593</td>\n",
       "      <td>0.991221</td>\n",
       "      <td>0.989062</td>\n",
       "      <td>0.988065</td>\n",
       "      <td>0.990109</td>\n",
       "      <td>0.990183</td>\n",
       "      <td>0.990183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>5923.000000</td>\n",
       "      <td>6742.000000</td>\n",
       "      <td>5958.000000</td>\n",
       "      <td>6131.000000</td>\n",
       "      <td>5842.000000</td>\n",
       "      <td>5421.000000</td>\n",
       "      <td>5918.000000</td>\n",
       "      <td>6265.000000</td>\n",
       "      <td>5851.000000</td>\n",
       "      <td>5949.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0            1            2            3            4  \\\n",
       "f1-score      0.993413     0.993100     0.989951     0.988468     0.988464   \n",
       "precision     0.993749     0.993469     0.987801     0.991306     0.986862   \n",
       "recall        0.993078     0.992732     0.992111     0.985647     0.990072   \n",
       "support    5923.000000  6742.000000  5958.000000  6131.000000  5842.000000   \n",
       "\n",
       "                     5            6            7            8            9  \\\n",
       "f1-score      0.989708     0.993334     0.989641     0.989315     0.986076   \n",
       "precision     0.994966     0.992078     0.988067     0.989569     0.984095   \n",
       "recall        0.984505     0.994593     0.991221     0.989062     0.988065   \n",
       "support    5421.000000  5918.000000  6265.000000  5851.000000  5949.000000   \n",
       "\n",
       "              macro avg     micro avg  weighted avg  \n",
       "f1-score       0.990147      0.990183      0.990184  \n",
       "precision      0.990196      0.990183      0.990194  \n",
       "recall         0.990109      0.990183      0.990183  \n",
       "support    60000.000000  60000.000000  60000.000000  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data = classification_report(y_train[0], y_pred_train, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data classification report\n",
    "Now, looking at the out of sample performance we can see that the network accurately classifies 0s the best while it classifies 9s and 5s not as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>micro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.983240</td>\n",
       "      <td>0.986784</td>\n",
       "      <td>0.975255</td>\n",
       "      <td>0.971513</td>\n",
       "      <td>0.973083</td>\n",
       "      <td>0.968768</td>\n",
       "      <td>0.973877</td>\n",
       "      <td>0.972317</td>\n",
       "      <td>0.971635</td>\n",
       "      <td>0.967359</td>\n",
       "      <td>0.974383</td>\n",
       "      <td>0.9746</td>\n",
       "      <td>0.974593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.978766</td>\n",
       "      <td>0.986784</td>\n",
       "      <td>0.976676</td>\n",
       "      <td>0.963938</td>\n",
       "      <td>0.970618</td>\n",
       "      <td>0.981588</td>\n",
       "      <td>0.974895</td>\n",
       "      <td>0.970902</td>\n",
       "      <td>0.976166</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>0.974578</td>\n",
       "      <td>0.9746</td>\n",
       "      <td>0.974638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.987755</td>\n",
       "      <td>0.986784</td>\n",
       "      <td>0.973837</td>\n",
       "      <td>0.979208</td>\n",
       "      <td>0.975560</td>\n",
       "      <td>0.956278</td>\n",
       "      <td>0.972860</td>\n",
       "      <td>0.973735</td>\n",
       "      <td>0.967146</td>\n",
       "      <td>0.969277</td>\n",
       "      <td>0.974244</td>\n",
       "      <td>0.9746</td>\n",
       "      <td>0.974600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>980.000000</td>\n",
       "      <td>1135.000000</td>\n",
       "      <td>1032.000000</td>\n",
       "      <td>1010.000000</td>\n",
       "      <td>982.000000</td>\n",
       "      <td>892.000000</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>1028.000000</td>\n",
       "      <td>974.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0            1            2            3           4  \\\n",
       "f1-score     0.983240     0.986784     0.975255     0.971513    0.973083   \n",
       "precision    0.978766     0.986784     0.976676     0.963938    0.970618   \n",
       "recall       0.987755     0.986784     0.973837     0.979208    0.975560   \n",
       "support    980.000000  1135.000000  1032.000000  1010.000000  982.000000   \n",
       "\n",
       "                    5           6            7           8            9  \\\n",
       "f1-score     0.968768    0.973877     0.972317    0.971635     0.967359   \n",
       "precision    0.981588    0.974895     0.970902    0.976166     0.965449   \n",
       "recall       0.956278    0.972860     0.973735    0.967146     0.969277   \n",
       "support    892.000000  958.000000  1028.000000  974.000000  1009.000000   \n",
       "\n",
       "              macro avg   micro avg  weighted avg  \n",
       "f1-score       0.974383      0.9746      0.974593  \n",
       "precision      0.974578      0.9746      0.974638  \n",
       "recall         0.974244      0.9746      0.974600  \n",
       "support    10000.000000  10000.0000  10000.000000  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data = classification_report(y_test[0], y_pred_test, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1) [Jonathan Weisberg - Building a Neural Network from Scratch: Part 1](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/)\n",
    "\n",
    "2) [Jonathan Weisberg - Building a Neural Network from Scratch: Part 2](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%202/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
